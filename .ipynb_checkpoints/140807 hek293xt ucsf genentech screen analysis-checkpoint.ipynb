{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MedianPolish:\n",
    "    \"\"\"Fits an additive model using Tukey's median polish algorithm\"\"\"\n",
    "\n",
    "    #note that the self argument is default that allows reference to an instance of the\n",
    "    #class that has been defined by the user\n",
    "    #no argument is actally specified to be passed to the self argument by the user;\n",
    "    #the first user specified argument\n",
    "    #goes into the first argument listed after self in the code of the class\n",
    "    def __init__(self, array): \n",
    "        \"\"\"Get numeric data from numpy ndarray to self.tbl, keep the original copy in tbl_org\"\"\"\n",
    "        #checks if the argument called array is actually a numerical array of the\n",
    "        #object type np.ndarray by using the comparison function isinstance\n",
    "        if isinstance(array, np.ndarray):\n",
    "            self.tbl_org = array\n",
    "            #tbl_org will allow you to compare the final polished matrix with row and column\n",
    "            #effects removed to the original unpolished matrix\n",
    "            self.tbl = self.tbl_org.copy()\n",
    "        else:\n",
    "            raise TypeError('Expected the argument to be a numpy.ndarray.')\n",
    "\n",
    "    @staticmethod\n",
    "    def csv_to_ndarray(fname): \n",
    "        \"\"\" Utility method for loading ndarray from .csv file\"\"\" \n",
    "        try:\n",
    "            #generates an array-like object of type np.ndarray\n",
    "            #from a comma separated values file\n",
    "            #MAKE SURE THAT NO CELL HAS EXTRA COMMAS!  THIS WILL BREAK THE CELL\n",
    "            #INTO 2 SEPARATE ELEMENTS IN THE ARRAY\n",
    "            return np.genfromtxt(fname, delimiter=\",\")\t\n",
    "        except Exception, e:\n",
    "            print \"Error loading file %s:\" % fname\n",
    "            raise\n",
    "\n",
    "    def median_polish(self, max_iterations, method):\n",
    "        \"\"\"\n",
    "            Implements Tukey's median polish alghoritm for additive models\n",
    "            method - default is median, alternative is mean. That would give us result equal ANOVA.\n",
    "        \"\"\"\n",
    "        \n",
    "        grand_effect = 0\n",
    "        median_row_effects = 0\n",
    "        median_col_effects = 0\n",
    "        #defines a vector that stores the row_effects during each iteration;\n",
    "        #the vector is initialized with zeros and length equal to the number of rows\n",
    "        #as determined by the function shape[0]\n",
    "        row_effects = np.zeros(shape=self.tbl.shape[0])\n",
    "        col_effects = np.zeros(shape=self.tbl.shape[1])\n",
    "\n",
    "        for i in range(max_iterations):\n",
    "            if method == 'median':\n",
    "                #note that np.median's second arg specifies along which axis to perform\n",
    "                #the median calculation, with axis = 1 being rows, and axis = 0 being columns\n",
    "                row_medians = np.median(self.tbl, 1)\n",
    "                #the next line keeps a running total of the row effects that have been\n",
    "                #subtracted out during the iterative polishing procedure\n",
    "                row_effects += row_medians\n",
    "                median_row_effects = np.median(row_effects)\n",
    "            elif method == 'average':\n",
    "                row_medians = np.average(self.tbl, 1) \n",
    "                row_effects += row_medians\n",
    "                median_row_effects = np.average(row_effects)\n",
    "            \n",
    "            #not sure what the grand_effect variable is keeping track of; it is\n",
    "            #returned but never used in later calculations\n",
    "            grand_effect += median_row_effects\n",
    "            \n",
    "            #the following line I have commented out, don't know why it is needed\n",
    "            #row_effects -= median_row_effects\n",
    "            \n",
    "            #the following line reshapes the the row_medians array into a column\n",
    "            #this reshaping is necessary because each element in row_medians\n",
    "            #is the median from each row, which needs to be subtracted from each row\n",
    "            #by reshaping into a column, can simply subtract this column from each\n",
    "            #column of the data matrix, self.tbl\n",
    "            #the np.newaxis function adds a new dimension, such that now the array\n",
    "            #is a matrix with n number of rows and 1 column; hence the array is now a\n",
    "            #column vector.\n",
    "            self.tbl -= row_medians[:, np.newaxis]\n",
    "\n",
    "            if method == 'median':\n",
    "                col_medians = np.median(self.tbl, 0) \n",
    "                col_effects += col_medians\n",
    "                median_col_effects = np.median(col_effects)\n",
    "            elif method == 'average':\n",
    "                col_medians = np.average(self.tbl, 0) \n",
    "                col_effects += col_medians\n",
    "                median_col_effects = np.average(col_effects)\n",
    "\n",
    "            #note that by default, arrays such as col_medians are formulated as a row vector\n",
    "            #for use in numerical calculations\n",
    "            self.tbl -= col_medians\n",
    "            \n",
    "            #the following line I have commented out, don't know why it is needed\n",
    "            #col_effects -= col_medians\n",
    "            \n",
    "            grand_effect += median_col_effects\n",
    "\n",
    "        return grand_effect, col_effects, row_effects , self.tbl, self.tbl_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12518  0.15896  0.11648  0.14447]\n",
      " [ 0.15452  0.16448  0.11236  0.13707]\n",
      " [ 0.18034  0.15022  0.1115   0.12896]\n",
      " [ 0.19405  0.15854  0.10054  0.12697]\n",
      " [ 0.18903  0.14414  0.11078  0.12864]\n",
      " [ 0.21098  0.16259  0.11563  0.12141]\n",
      " [ 0.18795  0.15622  0.0946   0.12566]\n",
      " [ 0.17864  0.14113  0.11051  0.10721]\n",
      " [ 0.19057  0.15762  0.1038   0.10608]\n",
      " [ 0.16356  0.14911  0.11928  0.11553]\n",
      " [ 0.16531  0.15511  0.10419  0.11855]\n",
      " [ 0.15692  0.14433  0.11908  0.11506]\n",
      " [ 0.15289  0.14654  0.11557  0.14982]\n",
      " [ 0.17917  0.15069  0.13138  0.13997]\n",
      " [ 0.1719   0.17128  0.13031  0.14255]\n",
      " [ 0.16596  0.15677  0.11751  0.12359]\n",
      " [ 0.05863  0.06591  0.08843  0.09778]\n",
      " [ 0.19231  0.16994  0.13865  0.12754]\n",
      " [ 0.18846  0.16022  0.11449  0.1166 ]\n",
      " [ 0.15584  0.12428  0.10883  0.09472]\n",
      " [ 0.1959   0.14815  0.11193  0.12543]\n",
      " [ 0.18804  0.1617   0.11267  0.1134 ]\n",
      " [ 0.18325  0.15995  0.12481  0.12295]\n",
      " [ 0.14957  0.15958  0.12236  0.11819]\n",
      " [ 0.14029  0.16414  0.13181  0.15084]\n",
      " [ 0.18782  0.16674  0.12869  0.14498]\n",
      " [ 0.1628   0.16233  0.14441  0.14533]\n",
      " [ 0.1717   0.16961  0.11804  0.1366 ]\n",
      " [ 0.14569  0.16586  0.14658  0.16075]\n",
      " [ 0.18062  0.14477  0.13637  0.14214]\n",
      " [ 0.18234  0.16247  0.11782  0.12512]\n",
      " [ 0.19941  0.14856  0.12233  0.15259]\n",
      " [ 0.16511  0.16058  0.12684  0.14879]\n",
      " [ 0.1522   0.17001  0.15441  0.12836]\n",
      " [ 0.17373  0.16225  0.12031  0.14868]\n",
      " [ 0.01016  0.0092   0.01286  0.01137]\n",
      " [ 0.16575  0.15542  0.1233   0.1502 ]\n",
      " [ 0.16982  0.13515  0.11378  0.13696]\n",
      " [ 0.15495  0.16997  0.11837  0.13859]\n",
      " [ 0.21609  0.16211  0.12177  0.13106]\n",
      " [ 0.16703  0.14681  0.11445  0.11645]\n",
      " [ 0.02982  0.02785  0.01982  0.01985]\n",
      " [ 0.16991  0.15518  0.12534  0.12014]\n",
      " [ 0.17964  0.13517  0.11646  0.11389]\n",
      " [ 0.13891  0.14756  0.08936  0.12274]\n",
      " [ 0.19974  0.16172  0.12144  0.12262]\n",
      " [ 0.18367  0.16766  0.13576  0.12155]\n",
      " [ 0.17632  0.17428  0.14728  0.1174 ]\n",
      " [ 0.10352  0.12452  0.12508  0.14998]\n",
      " [ 0.12357  0.15201  0.12381  0.13573]\n",
      " [ 0.12013  0.15125  0.13749  0.13149]\n",
      " [ 0.17308  0.16439  0.13216  0.12438]\n",
      " [ 0.1574   0.15727  0.12557  0.12015]\n",
      " [ 0.15622  0.12841  0.11904  0.11567]\n",
      " [ 0.10831  0.04927  0.1109   0.08868]\n",
      " [ 0.17935  0.17182  0.13122  0.11442]\n",
      " [ 0.18014  0.17017  0.12794  0.1195 ]\n",
      " [ 0.17827  0.18264  0.1394   0.11388]\n",
      " [ 0.17812  0.1357   0.12459  0.10318]\n",
      " [ 0.17953  0.15679  0.13492  0.14075]]\n"
     ]
    }
   ],
   "source": [
    "#perform missing data imputation on a screen-by-screen basis,\n",
    "#where a screen is considered to be all screens done in the same day/batch\n",
    "#of cells\n",
    "\n",
    "\"\"\"KNN (k nearest neighbors) missing data imputation using Euclidean distance to select k\n",
    "nearest neighbors and using weight averages for the estimation of the missing value.\n",
    "Missing values are imputed starting with the first column and going down each row in the\n",
    "first column, then repeating with the next column to the right in the data table.\"\"\"\n",
    "#chose 10 nearest neighbors because in a given screen not likely has more than 10 or so hits\n",
    "#and so only about 10 or so neighboring hits would be informative for imputing a putative\n",
    "#hit that is missing a value.  for putative non-hits, there are many neighbors with similar\n",
    "#values and so 10 is still a good number\n",
    "K_NEAREST_NEIGHBORS = 10\n",
    "\n",
    "NUM_REPL = 4\n",
    "\n",
    "data_file = open('/Users/markfang/Dropbox/UCSD Grad work/RNA-Yeo Lab/Data - analyses/SG project/screening/granule area to nuclear area analyses of large screens/140807 hek293xt ucsf genentech/before.csv', 'rU')\n",
    "data_table = np.genfromtxt(data_file, delimiter=',')\n",
    "\n",
    "#the actual data is listed in the following rows and columns; the rest are\n",
    "#data table label strings\n",
    "FIRSTROW = 4\n",
    "LASTROW = 64\n",
    "FIRSTCOL = 1\n",
    "LASTCOL = 5\n",
    "\n",
    "data_table = data_table[FIRSTROW:LASTROW, FIRSTCOL:LASTCOL]\n",
    "\n",
    "original_numrows = data_table.shape[0]\n",
    "original_numcols = data_table.shape[1]\n",
    "\n",
    "data_table = np.reshape(np.ravel(data_table, order = 'F'), (original_numrows * original_numcols / NUM_REPL, NUM_REPL), order = 'F')\n",
    "\n",
    "numrows = data_table.shape[0]\n",
    "numcols = data_table.shape[1]\n",
    "\n",
    "for j in range(numcols):\n",
    "    for i in range(numrows):\n",
    "        #find missing values by checking for 'nan' value in ndarray\n",
    "        if np.isnan(data_table[i][j]):\n",
    "            #for determining Euclidean distances from the small molecule compound which is\n",
    "            #missing value in 1 replicate to all the other vectors containing data for\n",
    "            #all the other small molecule compounds,\n",
    "            #we ignore the values in the replicate in which there is the missing value\n",
    "            other_vectors = np.delete(data_table, j, axis = 1)\n",
    "            \n",
    "            #initialize the small molecule compound for which\n",
    "            #there is a missing value for 1 replicate; this vector\n",
    "            #will be used as the basis to compare Euclidean distance to the other_vectors\n",
    "            vector_missingval = other_vectors[i:(i + 1), 0:numcols]\n",
    "            \n",
    "            #initialize a vector containing the data in the same repl in which there is\n",
    "            #the missing value; the missing value will be imputed by a weighted average\n",
    "            #of the other data in this replicate\n",
    "            estimating_vals = data_table[0:numrows, j:(j + 1)]\n",
    "            \n",
    "            other_vectors = np.delete(other_vectors, i, axis = 0)\n",
    "            estimating_vals = np.delete(estimating_vals, i, axis = 0)\n",
    "            \n",
    "            nan_estimator_flag = 1\n",
    "            \n",
    "            #remove other rows in which the estimating value is also missing\n",
    "            while(nan_estimator_flag == 1):\n",
    "                nan_estimator_flag = 0\n",
    "                for x in range(len(estimating_vals)):\n",
    "                    if (np.isnan(estimating_vals[x])):\n",
    "                        nan_estimator_flag = 1\n",
    "                        row_with_nan = x\n",
    "                \n",
    "                if(nan_estimator_flag == 1):\n",
    "                    other_vectors = np.delete(other_vectors, row_with_nan, axis = 0)\n",
    "                    estimating_vals = np.delete(estimating_vals, row_with_nan, axis = 0)\n",
    "            \n",
    "            #the following vector will store the Euclidean distances\n",
    "            euclid_dist = np.zeros((other_vectors.shape[0], 1))\n",
    "\n",
    "            for k in range(other_vectors.shape[0]):\n",
    "                flag_nanvector = 0  \n",
    "                \n",
    "                for l in range(other_vectors.shape[1]):\n",
    "                    if (np.isnan(other_vectors[k][l]) == False) and (np.isnan(vector_missingval[0][l]) == False):\n",
    "                        #calculate Euclidean distance as defined in Troyanskaya et al (2001)\n",
    "                        euclid_dist[k][0] += (other_vectors[k][l] - vector_missingval[0][l]) ** 2\n",
    "                        \n",
    "                        flag_nanvector = 1\n",
    "                \n",
    "                if flag_nanvector == 0:\n",
    "                    #nans are always ranked last\n",
    "                    euclid_dist[k][0] = data_table[i][j]\n",
    "            \n",
    "            #the similarity score calculated below and as defined in Troyanskaya et al (2001)\n",
    "            #will serve as the weights for imputing the missing value based on weighted\n",
    "            #average of the remaining - observed - data in the replicate missing the value\n",
    "            similarity = 1 / euclid_dist\n",
    "            \n",
    "            #rank the other small molecule compounds in terms of their Euclidean distance\n",
    "            #from the small molecule compound with the missing value.  select the k nearest\n",
    "            #neighbors in terms of Euclidean distance\n",
    "            simil_rank = scipy.stats.rankdata(euclid_dist, method = \"ordinal\")\n",
    "            \n",
    "            sum_simil = 0\n",
    "            weighted_avg = 0\n",
    "            for m in range(other_vectors.shape[0]):\n",
    "                if simil_rank[m] <= K_NEAREST_NEIGHBORS:\n",
    "                    sum_simil += similarity[m]\n",
    "                    weighted_avg += similarity[m] * estimating_vals[m]\n",
    "            data_table[i][j] = weighted_avg/sum_simil\n",
    "\n",
    "#reform the original data table, now with the missing values filled in\n",
    "data_table = np.reshape(np.ravel(data_table, order = 'F'), (original_numrows, original_numcols), order = 'F')\n",
    "print(data_table)\n",
    "\n",
    "#with open ('/Users/markfang/Dropbox/UCSD Grad work/RNA-Yeo Lab/Data - analyses/SG project/screening/granule area to nuclear area analyses of large screens/140807 hek293xt ucsf genentech/missingval filled.csv','w') as filled_outputfile:\n",
    "#    writer = csv.writer(filled_outputfile, lineterminator = '\\n')\n",
    "#    writer.writerows(data_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.13096   0.73953   0.35106  -0.81483]\n",
      " [ -2.26067   2.60369   0.13588   0.56525]\n",
      " [  0.62043  -1.0855   -2.25384  -0.85473]\n",
      " [  0.76274  -0.81488  -2.51513   1.02265]\n",
      " [  1.97082  -0.61762   0.18065   3.18478]\n",
      " [  2.02617   5.46162   0.        0.     ]\n",
      " [  0.        0.19318  -1.35964   4.46803]\n",
      " [ -0.63898   0.        0.       -1.59101]\n",
      " [  0.        0.9145   -0.59251  -5.25278]\n",
      " [ -3.11269  -3.00935   0.        0.44393]\n",
      " [ -2.37267  -0.       -1.04005  -0.24486]\n",
      " [  0.       -2.85028   0.       -0.00148]\n",
      " [  0.34882  -1.64171  -1.01912   0.     ]\n",
      " [  0.87239  -0.07907   3.09428   0.48155]\n",
      " [  0.        3.85651   0.6593    2.98014]\n",
      " [ -2.08731  -0.86779   0.       -1.36425]\n",
      " [-12.48266 -17.3974   -5.84903  -9.27373]\n",
      " [  0.24572   7.40335   3.82877   1.09944]\n",
      " [  0.39518   1.40284   1.7881    0.     ]\n",
      " [ -2.88793  -3.35053  -1.53793  -7.31703]\n",
      " [  0.94222  -0.82242   0.        0.69262]\n",
      " [  0.        0.07907  -2.60688  -1.48321]\n",
      " [  0.        1.39455   2.26564   0.21958]\n",
      " [ -0.49738   0.82193  -0.45948   0.     ]\n",
      " [  0.        0.30943   0.       -3.62545]\n",
      " [  2.93416   1.5343    0.       -1.68166]\n",
      " [  0.0482    0.        1.21356  -0.00091]\n",
      " [ -0.35572   0.04458  -2.39521  -0.59538]\n",
      " [ -1.52713   2.5704    4.27393   9.81046]\n",
      " [  0.        0.        0.8234    2.45184]\n",
      " [  0.7817   -0.00303   0.       -0.87472]\n",
      " [  3.13449   0.06323  -1.1159    9.89643]\n",
      " [ -1.46971   0.        0.73027   5.25747]\n",
      " [ -2.98452   0.        3.94935   0.     ]\n",
      " [ -0.00009   0.       -1.22313   5.65224]\n",
      " [-15.22966 -33.97312 -26.75671 -43.15898]\n",
      " [  1.99567   0.        0.69845   0.43157]\n",
      " [  0.       -3.77837  -0.69053  -0.33082]\n",
      " [ -1.73378   3.26858  -1.89538   1.81722]\n",
      " [  3.7874    0.        0.96224   1.66438]\n",
      " [  0.00111   0.       -0.15884  -2.13901]\n",
      " [-17.99727 -23.98008 -21.94763 -38.08428]\n",
      " [ -1.52111   0.        4.18208   1.5884 ]\n",
      " [  0.       -1.26889   0.15884   0.     ]\n",
      " [ -5.33363  -1.25038  -4.86533   0.     ]\n",
      " [  1.51645  -0.21675  -0.66372   2.18881]\n",
      " [  0.23608   2.77913   4.6812    0.     ]\n",
      " [  2.72541   3.73577   4.99162   0.00028]\n",
      " [ -3.97076  -6.66986  -1.17211   1.13232]\n",
      " [ -4.15483   0.       -0.76996   0.     ]\n",
      " [ -4.59247  -0.73745   0.        0.     ]\n",
      " [  0.        0.58941   0.96329   0.     ]\n",
      " [  0.        2.37734   0.        0.     ]\n",
      " [ -2.56891  -1.89212  -2.64952  -2.17626]\n",
      " [ -7.41697 -23.08028  -1.21008  -9.1612 ]\n",
      " [  1.05778   6.83925   1.10698   0.97735]\n",
      " [  0.43405   3.78432   1.26049  -0.40741]\n",
      " [  0.17237   4.44885   0.98088  -0.23243]\n",
      " [  0.69698  -4.12154   0.       -5.95129]\n",
      " [  4.18108   0.        0.04872   9.34279]]\n"
     ]
    }
   ],
   "source": [
    "#perform polish and calculate b scores on a plate-by-plate basis\n",
    "#where each plate's data is listed as a separate column in the original csv\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #number of iterations to run Tukey's two median polish\n",
    "    NUM_POLISH_ITER = 10\n",
    "    \n",
    "    #number of plates, rows, columns\n",
    "    PLATES = 4\n",
    "    PLATE_ROWS = 5\n",
    "    PLATE_COLS = 12\n",
    "    \n",
    "    #makes output more legible for debugging\n",
    "    np.set_printoptions(precision = 5, suppress = True)\n",
    "\n",
    "    num_rows = data_table.shape[0]\n",
    "    num_cols = data_table.shape[1]\n",
    "    \n",
    "    #the data table needs to be reshaped into plates with rows and columns\n",
    "    #this way I can run the polish on each of the plates.  the next few lines\n",
    "    #accomplish this reshaping\n",
    "    #note that by entering a 1-tuple for size, reshape will make a 1D array; no\n",
    "    #need to actually specify that the array has 1 row or 1 col, as functionally\n",
    "    #np will treat that as a 2D array with 1 row or 1 col\n",
    "    data_table = np.reshape(data_table, (num_rows * num_cols), order = 'F')\n",
    "\n",
    "    #reshape into data organized by plate, row, col\n",
    "    data_table = np.reshape(data_table, (PLATES, PLATE_ROWS, PLATE_COLS), order = 'C')\n",
    "\n",
    "    bscores = []\n",
    "    bscores_printed = np.zeros((PLATE_ROWS * PLATE_COLS, PLATES))\n",
    "    resid_printed = np.zeros((PLATE_ROWS * PLATE_COLS, PLATES))\n",
    "    \n",
    "    #perform polish and calculate b scores on a plate-by-plate basis\n",
    "    for i in xrange(0, PLATES):\n",
    "    \n",
    "        #iterate over each plate and perform the median polish\n",
    "        arr = data_table[i, :, :]\n",
    "\n",
    "        tbl_avg = np.average(arr)\n",
    "        #subtract out the average for each plate, thus normalizing out plate effects\n",
    "        arr -= tbl_avg\n",
    "        #pass each plate's data matrix into the MedianPolish object\n",
    "        mp = MedianPolish(arr)\n",
    "\n",
    "        #first argument indicates number of iterations to be run\n",
    "        #ce is an ndarray storing the column effects after n iterations of polishing\n",
    "        #re is an ndarray storing the row effects after n iterations of polishing\n",
    "        #resid is the data table that has been polished to remove \n",
    "        ge, ce, re, resid, tbl_org =  mp.median_polish(NUM_POLISH_ITER, \"median\") \n",
    "\n",
    "        re_reshape = re[:, np.newaxis]\n",
    "        \n",
    "        #the tbl_org returned by mmp.median_polish has had the tbl_avg subtracted,\n",
    "        #so to get the initial data table back need to add the tbl_avg back\n",
    "        tbl_org += tbl_avg\n",
    "\n",
    "        #the next few lines compute the median absolute deviation\n",
    "        #MAD = median(|x - median(x)|)\n",
    "        tbl_resid_minusmedians = resid - np.median(resid)\n",
    "        median_absdev = np.median(np.absolute(tbl_resid_minusmedians))\n",
    "        \n",
    "        #find the b scores of the plate\n",
    "        tbl_bscore = resid / median_absdev\n",
    "        \n",
    "        tbl_bscore_toprint = np.reshape(np.ravel(tbl_bscore, order = 'C'), (PLATE_ROWS * PLATE_COLS), order = 'F')\n",
    "        bscores_printed[:, i] = tbl_bscore_toprint\n",
    "        \n",
    "        resid_toprint = np.reshape(np.ravel(resid, order = 'C'), (PLATE_ROWS * PLATE_COLS), order = 'F')\n",
    "        resid_printed[:, i] = resid_toprint\n",
    "        \n",
    "        #convert tbl_bscore from ndarray to simple list\n",
    "        tbl_bscore = tbl_bscore.tolist()\n",
    "        \n",
    "        #collect the b scores of each plate into the array bscores\n",
    "        bscores.append(tbl_bscore)\n",
    "    \n",
    "    bscores = np.asarray(bscores)\n",
    "    \n",
    "    print(bscores_printed)\n",
    "    \n",
    "#    with open ('/Users/markfang/Dropbox/UCSD Grad work/RNA-Yeo Lab/Data - analyses/SG project/screening/granule area to nuclear area analyses of large screens/140807 hek293xt ucsf genentech/b scores.csv','w') as bscores_outputfile:\n",
    "#        writer = csv.writer(bscores_outputfile, lineterminator = '\\n')\n",
    "#        writer.writerows(bscores_printed)\n",
    "        \n",
    "#    with open ('/Users/markfang/Dropbox/UCSD Grad work/RNA-Yeo Lab/Data - analyses/SG project/screening/granule area to nuclear area analyses of large screens/140807 hek293xt ucsf genentech/after.csv.csv','w') as afterpolish_outputfile:\n",
    "#        writer = csv.writer(afterpolish_outputfile, lineterminator = '\\n')\n",
    "#        writer.writerows(resid_printed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  5]\n",
      " [ 1  2  8]\n",
      " [ 1  3 12]\n",
      " [ 1  4  6]\n",
      " [ 1  4  9]\n",
      " [ 1  5  7]]\n",
      "[[ 1  3  5]\n",
      " [ 1  4 12]]\n"
     ]
    }
   ],
   "source": [
    "#select hits based on median and median absolute deviation, on a screen-by-screen\n",
    "#basis, where a screen is considered to be all screens done in the same day/batch\n",
    "#of cells\n",
    "    \n",
    "#1.4826 is scaling constant to make 1 MAD comparable in magnitude to 1 SD\n",
    "#see (chung, strulovici et al 2007)\n",
    "MAD_SCALING_CONST = 1.4826\n",
    "INDEX_OFFSET = 1\n",
    "#2 MAD corresponds to false positive rate of 0.023 under a normal distribution\n",
    "THRESHOLD = 2\n",
    "    \n",
    "bscores_median = np.median(bscores)\n",
    "\n",
    "bscores_copy = bscores\n",
    "#subtract the overall median; this is later used to calculate the MAD\n",
    "for i in range(0, PLATES):\n",
    "    for j in range(0, PLATE_ROWS):\n",
    "        for k in range(0, PLATE_COLS):\n",
    "            bscores_copy[i][j][k] -= bscores_median\n",
    "\n",
    "mad = MAD_SCALING_CONST * np.median(np.absolute(bscores_copy))\n",
    "    \n",
    "upper_threshold = bscores_median + THRESHOLD * mad\n",
    "lower_threshold = bscores_median - THRESHOLD * mad\n",
    "\n",
    "hits_mad = []\n",
    "sg_enhancers_mad = []\n",
    "    \n",
    "unique_plates = PLATES / NUM_REPL\n",
    "\n",
    "for i in range(0, unique_plates):\n",
    "    for j in range(0, PLATE_ROWS):\n",
    "        for k in range(0, PLATE_COLS):\n",
    "            #find median of replicates, then check if it is +/- 3 MAD\n",
    "            arr_to_find_median = []\n",
    "            \n",
    "            for l in range(0, NUM_REPL):\n",
    "                arr_to_find_median.append(bscores[i + (l * unique_plates)][j][k])\n",
    "            \n",
    "            median_ofrepl = np.median(arr_to_find_median)\n",
    "            \n",
    "            if median_ofrepl < lower_threshold:\n",
    "                #collect plate, row, column coordinates of hits\n",
    "                #indexed from 1 rather than 0\n",
    "                hits_mad.append(i + INDEX_OFFSET)\n",
    "                hits_mad.append(j + INDEX_OFFSET)\n",
    "                hits_mad.append(k + INDEX_OFFSET)\n",
    "            if median_ofrepl > upper_threshold:\n",
    "                sg_enhancers_mad.append(i + INDEX_OFFSET)\n",
    "                sg_enhancers_mad.append(j + INDEX_OFFSET)\n",
    "                sg_enhancers_mad.append(k + INDEX_OFFSET)\n",
    "\n",
    "\n",
    "len_hits_mad = len(hits_mad)\n",
    "hits_mad = np.asarray(hits_mad)\n",
    "\n",
    "#reshape into ordered triples giving the plate, row, and column coordinates for hits\n",
    "hits_mad = np.reshape(hits_mad, (len_hits_mad / 3, 3), order = 'C')\n",
    "print(hits_mad)\n",
    "\n",
    "len_sg_enhancers_mad = len(sg_enhancers_mad)\n",
    "sg_enhancers_mad = np.asarray(sg_enhancers_mad)\n",
    "sg_enhancers_mad = np.reshape(sg_enhancers_mad, (len_sg_enhancers_mad / 3, 3), order = 'C')\n",
    "print(sg_enhancers_mad)\n",
    "    \n",
    "#with open ('/Users/markfang/Dropbox/UCSD Grad work/RNA-Yeo Lab/Data - analyses/SG project/screening/granule area to nuclear area analyses of large screens/140807 hek293xt ucsf genentech/hits mad.csv','w') as hits_mad_outputfile:\n",
    "#    writer = csv.writer(hits_mad_outputfile, lineterminator = '\\n')\n",
    "#    writer.writerows(hits_mad)\n",
    "        \n",
    "#with open ('/Users/markfang/Dropbox/UCSD Grad work/RNA-Yeo Lab/Data - analyses/SG project/screening/granule area to nuclear area analyses of large screens/140807 hek293xt ucsf genentech/sg enhancers mad.csv','w') as sgenh_mad_outputfile:\n",
    "#    writer = csv.writer(sgenh_mad_outputfile, lineterminator = '\\n')\n",
    "#    writer.writerows(sg_enhancers_mad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2.4554167656848711, 0.075945226885788419, 2.8052508189563694)\n",
      "1.22770838284\n",
      "0.290357521729\n",
      "3.0329966126\n",
      "1.08118553682\n",
      "(3, 2.4553255529192048, 0.027073994976722103, 0.9999769801714532)\n",
      "3.0329966126\n"
     ]
    }
   ],
   "source": [
    "#fit variances to random variance model (inverse gamma distributed) on a screen-by-screen\n",
    "#basis, where a screen is considered to be all screens done in the same day/batch\n",
    "#of cells\n",
    "\n",
    "#number of experimental and control groups\n",
    "NUM_GROUPS = 1\n",
    "\n",
    "variances = []\n",
    "averages = []\n",
    "\n",
    "for i in range(0, unique_plates):\n",
    "    for j in range (0, PLATE_ROWS):\n",
    "        for k in range (0, PLATE_COLS):\n",
    "            sum_of_repl = 0\n",
    "            sum_of_var = 0\n",
    "            \n",
    "            for l in range(0, NUM_REPL):\n",
    "                sum_of_repl += (bscores[i + (l * unique_plates)][j][k])\n",
    "            \n",
    "            sample_avg = sum_of_repl / NUM_REPL\n",
    "\n",
    "            for m in range(0, NUM_REPL):\n",
    "                sum_of_var += ((bscores[i + (m * unique_plates)][j][k]) - sample_avg) ** 2\n",
    "\n",
    "            sample_variance = sum_of_var / (NUM_REPL - 1)\n",
    "            \n",
    "            variances.append(sample_variance)\n",
    "            averages.append(sample_avg)\n",
    "\n",
    "#according to paper wright and simon 2003, the sample variances multipled by two constants\n",
    "#a and b follow an F distribution with parameters (n-k) and 2a, where n is number of\n",
    "#replicates, k is number of group (experimental, control, etc)\n",
    "#in my data, I have duplicates and 1 group, so n = 2, k = 1.\n",
    "param = scipy.stats.f.fit(variances, f0 = NUM_REPL - NUM_GROUPS)\n",
    "print(param)\n",
    "\n",
    "#after fitting we want to find the value of a and b, since these are the parameters for\n",
    "#the putative inverse gamma distribution that is the true distribution of the variances\n",
    "#of the small molecule screen.  finding a and b will help us specify the inverse\n",
    "#gamma distribution, which will improve the power of our t tests (wright and simon 2003)\n",
    "#find parameter a: since the fitted distribution has parameters (n-k) and 2a, we can\n",
    "#take the second parameter and divide by 2 to get a\n",
    "invgammaparam_a = param[1] / 2\n",
    "\n",
    "#we fit an F distribution to our variances, and we see that the scaling s is stored in the\n",
    "#fourth parameter.  a*b*variances fits to an F distribution with area under the curve = 1\n",
    "#since F is a probability distribution (scaling = 1)\n",
    "#thus when we simply fit our variances to an F distribution,\n",
    "#we may get a scaling s =/= 1 (area under the curve not equal 1)\n",
    "#since multiplying variates by a constant changes the scaling of the fitted F distribution\n",
    "#we can figure out what a*b is by knowing that multiplying the variances\n",
    "#by a*b brings the scaling up to 1; hence a*b equals the multiplicative inverse\n",
    "#of the current scaling.  from here we can find b because we already have a\n",
    "invgammaparam_b = (1 / param[3]) / invgammaparam_a\n",
    "print(invgammaparam_a)\n",
    "print(invgammaparam_b)\n",
    "\n",
    "print(variances[0])\n",
    "new_variances = [(invgammaparam_a * invgammaparam_b * x) for x in variances]\n",
    "print(new_variances[0])\n",
    "param = scipy.stats.f.fit(new_variances, f0 = NUM_REPL - NUM_GROUPS)\n",
    "print(param)\n",
    "print(variances[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0003833881657073398, 1.0, 4.0, 6.0], [0.0007623321141911842, 1.0, 3.0, 12.0], [0.0014710499847934767, 1.0, 2.0, 5.0], [0.008425706006812516, 1.0, 5.0, 6.0], [0.015033361837346604, 1.0, 2.0, 8.0], [0.028283654412188608, 1.0, 4.0, 12.0], [0.02882514774033162, 1.0, 5.0, 7.0], [0.048195887127079395, 1.0, 4.0, 9.0], [0.05891867592958349, 1.0, 2.0, 6.0], [0.08273575926883599, 1.0, 2.0, 3.0], [0.08903401489630958, 1.0, 3.0, 5.0], [0.08987726717066837, 1.0, 5.0, 8.0], [0.10283827762056817, 1.0, 4.0, 4.0], [0.10290899359706082, 1.0, 5.0, 12.0], [0.10506813785517975, 1.0, 5.0, 1.0], [0.10719593880771466, 1.0, 4.0, 11.0], [0.13013775951569348, 1.0, 5.0, 11.0], [0.14711501909558014, 1.0, 1.0, 6.0], [0.15219180228727353, 1.0, 2.0, 4.0], [0.17165356575696067, 1.0, 1.0, 10.0], [0.17380634640738649, 1.0, 3.0, 8.0], [0.20245619879550653, 1.0, 5.0, 9.0], [0.20675046058102117, 1.0, 2.0, 2.0], [0.21148438454995375, 1.0, 2.0, 11.0], [0.21525135206321055, 1.0, 4.0, 2.0], [0.21674761902454473, 1.0, 2.0, 7.0], [0.21916432552040022, 1.0, 1.0, 5.0], [0.22019190889046875, 1.0, 5.0, 10.0], [0.2274210765774088, 1.0, 2.0, 10.0], [0.23186786992299854, 1.0, 5.0, 3.0], [0.23619641913912806, 1.0, 5.0, 2.0], [0.23641884509797853, 1.0, 1.0, 11.0], [0.2607554041533958, 1.0, 1.0, 3.0], [0.27611626972123804, 1.0, 4.0, 1.0], [0.2808422254284157, 1.0, 3.0, 4.0], [0.29474456474276894, 1.0, 3.0, 6.0], [0.3343817808765946, 1.0, 1.0, 9.0], [0.36023522691110693, 1.0, 4.0, 7.0], [0.3897045992325847, 1.0, 4.0, 10.0], [0.3911928053726176, 1.0, 3.0, 9.0], [0.3921646106384247, 1.0, 3.0, 1.0], [0.3943134683755031, 1.0, 1.0, 12.0], [0.4115551069078083, 1.0, 1.0, 8.0], [0.4157750823350451, 1.0, 2.0, 1.0], [0.42142833673112595, 1.0, 3.0, 11.0], [0.4356091098592918, 1.0, 4.0, 5.0], [0.4392920182686148, 1.0, 1.0, 1.0], [0.4401344771264159, 1.0, 5.0, 5.0], [0.4810096321016295, 1.0, 1.0, 7.0], [0.48330151544588273, 1.0, 3.0, 2.0], [0.5364224275676005, 1.0, 5.0, 4.0], [0.6223531211116029, 1.0, 3.0, 3.0], [0.6584460345934748, 1.0, 1.0, 4.0], [0.6684861959170216, 1.0, 4.0, 8.0], [0.7553188942746225, 1.0, 4.0, 3.0], [0.760555206601504, 1.0, 2.0, 9.0], [0.789126245754245, 1.0, 1.0, 2.0], [0.8473450585781533, 1.0, 3.0, 10.0], [0.9575996343224857, 1.0, 2.0, 12.0], [0.9702556080506451, 1.0, 3.0, 7.0]]\n",
      "[-0.83394  0.28091 -1.25442 -0.46717  1.38798  1.68897  0.75599 -0.88886\n",
      " -1.05861 -1.57306 -1.32998 -0.92443 -0.88032  1.43238  2.12135 -1.66351\n",
      " -5.9028   2.38027  1.39644 -3.49709  0.32043 -1.35972  1.41517 -0.05565\n",
      " -0.92895  0.75185  0.52172 -1.19666  2.06593  1.15877 -0.03903  1.56367\n",
      "  0.931    0.20185  0.869   -6.7533   1.20991 -1.40172  0.32774  1.95745\n",
      " -0.84108 -7.74699  0.99852 -0.45228 -2.53602  0.93415  1.92628  2.96173\n",
      " -1.94134 -1.33071 -1.34489  0.65946  0.83232 -4.02451 -2.94622  2.05882\n",
      "  1.44832  1.38441 -1.78089  1.95693]\n",
      "[ -0.7138    0.26104  -0.89341  -0.38615   1.17966   1.87195   0.82539\n",
      "  -0.5575   -1.2327   -1.41953  -0.91439  -0.71294  -0.578     1.09229\n",
      "   1.87399  -1.07983 -11.2507    3.14432   0.89653  -3.77335   0.2031\n",
      "  -1.00276   0.96994  -0.03373  -0.82901   0.6967    0.31521  -0.82543\n",
      "   3.78191   0.81881  -0.02402   2.99456   1.12951   0.24121   1.10725\n",
      " -29.77962   0.78142  -1.19993   0.36416   1.6035   -0.57419 -25.50231\n",
      "   1.06234  -0.27751  -2.86233   0.7062    1.9241    2.86327  -2.6701\n",
      "  -1.2312   -1.33248   0.38817   0.59433  -2.3217  -10.21714   2.49534\n",
      "   1.26786   1.34242  -2.34396   3.39315]\n"
     ]
    }
   ],
   "source": [
    "#select hits based on t tests (under random variance model), on a screen-by-screen\n",
    "#basis, where a screen is considered to be all screens done in the same day/batch\n",
    "#of cells\n",
    "\n",
    "#convert lists to numpy objects ndarrays to be able to easily perform math operations\n",
    "variances = np.asarray(variances)\n",
    "averages = np.asarray(averages)\n",
    "\n",
    "#variances that have been fitted to the inverse gamma distribution\n",
    "rvm_variances = ((NUM_REPL - 1) * variances + 2 * invgammaparam_a * (1 / (invgammaparam_a * invgammaparam_b))) / ((NUM_REPL - 1) + 2 * invgammaparam_a)          \n",
    "\n",
    "#calculate std dev from the variances that have been fitted to the inverse gamma distribution\n",
    "denominator = np.sqrt(rvm_variances / NUM_REPL)\n",
    "\n",
    "#calculate t statistic for each compound, using the rvm_variances\n",
    "t_stats = (averages - 0) / denominator\n",
    "\n",
    "len_t_stats = t_stats.shape[0]\n",
    "\n",
    "p_val = []\n",
    "df = NUM_REPL - 1 + (2 * invgammaparam_a)\n",
    "\n",
    "#p values for 2 tailed t tests\n",
    "for i in range(0, len_t_stats):\n",
    "    if t_stats[i] <= 0:\n",
    "        prob = scipy.stats.t.cdf(t_stats[i], df)\n",
    "        prob *= 2\n",
    "        p_val.append(prob)\n",
    "    else:\n",
    "        prob = scipy.stats.t.sf(t_stats[i], df)\n",
    "        prob *= 2\n",
    "        p_val.append(prob)\n",
    "\n",
    "p_val = np.asarray(p_val)\n",
    "p_val = p_val[:, np.newaxis]\n",
    "\n",
    "coordinates = []\n",
    "\n",
    "#list out the plate, row, and col coordinates to be concatenated with the p values\n",
    "#this helps keep track of where each p value came from in the physical location\n",
    "#on the plates after the p values are sorted in order to do FDR controlling\n",
    "#such as benjamini hochberg\n",
    "#these are indexed from 1 not 0 for ease of interpretation (i.e. plate 1 rather than plate 0)\n",
    "for i in range(0, unique_plates):\n",
    "    for j in range(0, PLATE_ROWS):\n",
    "        for k in range(0, PLATE_COLS):\n",
    "            coordinates.append(i + INDEX_OFFSET)\n",
    "            coordinates.append(j + INDEX_OFFSET)\n",
    "            coordinates.append(k + INDEX_OFFSET)\n",
    "\n",
    "coordinates = np.asarray(coordinates)\n",
    "len_coord = len(coordinates)\n",
    "coordinates = np.reshape(coordinates, (len_coord / 3, 3), order = 'C')\n",
    "\n",
    "p_val_coord = np.concatenate((p_val, coordinates), axis = 1)\n",
    "p_val_coord = np.ndarray.tolist(p_val_coord)\n",
    "\n",
    "p_val_coord.sort()\n",
    "print(p_val_coord)\n",
    "\n",
    "print(t_stats)\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[[0.0003833881657073398, 1.0, 4.0, 6.0], [0.0007623321141911842, 1.0, 3.0, 12.0], [0.0014710499847934767, 1.0, 2.0, 5.0], [0.008425706006812516, 1.0, 5.0, 6.0], [0.015033361837346604, 1.0, 2.0, 8.0], [0.02882514774033162, 1.0, 5.0, 7.0], [0.048195887127079395, 1.0, 4.0, 9.0]]\n",
      "1\n",
      "[[0.028283654412188608, 1.0, 4.0, 12.0]]\n"
     ]
    }
   ],
   "source": [
    "#select hits based on t tests (under random variance model), on a screen-by-screen\n",
    "#basis, where a screen is considered to be all screens done in the same day/batch\n",
    "#of cells\n",
    "\n",
    "#ignoring multiple hypothesis testing and therefore not controlling for FDR\n",
    "SIGMA = 0.05\n",
    "hits_t_test = []\n",
    "sg_enhancers_t_test = []\n",
    "\n",
    "len_p_val_coord = len(p_val_coord)\n",
    "\n",
    "for i in range(0, len_p_val_coord):\n",
    "    if p_val_coord[i][0] < SIGMA:\n",
    "        plate_position = (p_val_coord[i][1] - 1) * PLATE_ROWS * PLATE_COLS\n",
    "        row_position = (p_val_coord[i][2] - 1) * PLATE_COLS\n",
    "        col_position = p_val_coord[i][3] - 1\n",
    "        if t_stats[plate_position + row_position + col_position] <= 0:\n",
    "            hits_t_test.append(p_val_coord[i])\n",
    "        else:\n",
    "            sg_enhancers_t_test.append(p_val_coord[i])\n",
    "print(len(hits_t_test))        \n",
    "print(hits_t_test)\n",
    "print(len(sg_enhancers_t_test))\n",
    "print(sg_enhancers_t_test)\n",
    "\n",
    "#with open ('/Users/markfang/Dropbox/UCSD Grad work/RNA-Yeo Lab/Data - analyses/SG project/screening/granule area to nuclear area analyses of large screens/140807 hek293xt ucsf genentech/hits ttest.csv','w') as hits_t_test_outputfile:\n",
    "#    writer = csv.writer(hits_t_test_outputfile, lineterminator = '\\n')\n",
    "#    writer.writerows(hits_t_test)\n",
    "        \n",
    "#with open ('/Users/markfang/Dropbox/UCSD Grad work/RNA-Yeo Lab/Data - analyses/SG project/screening/granule area to nuclear area analyses of large screens/140807 hek293xt ucsf genentech/sg enhancers ttest.csv','w') as sgenh_t_test_outputfile:\n",
    "#    writer = csv.writer(sgenh_t_test_outputfile, lineterminator = '\\n')\n",
    "#    writer.writerows(sg_enhancers_t_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000833333333333\n",
      "[[0.0003833881657073398, 1.0, 4.0, 6.0], [0.0007623321141911842, 1.0, 3.0, 12.0], [0.0014710499847934767, 1.0, 2.0, 5.0]]\n"
     ]
    }
   ],
   "source": [
    "#benjamini hochberg method to control FDR\n",
    "FDR = 0.05\n",
    "k = 1.0\n",
    "i = 0\n",
    "m = unique_plates * PLATE_ROWS * PLATE_COLS\n",
    "\n",
    "hits_fdr = []\n",
    "\n",
    "print(k / m * FDR)\n",
    "\n",
    "while p_val_coord[i][0] < (k / m * FDR):\n",
    "    hit = p_val_coord[i]\n",
    "    hits_fdr.append(hit)\n",
    "    i += 1\n",
    "    k += 1.0\n",
    "    \n",
    "print(hits_fdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"#with open ('/Users/markfang/Dropbox/UCSD Grad work/RNA-Yeo Lab/Data - analyses/SG project/screening/granule area to nuclear area analyses of large screens/140807 hek293xt ucsf genentech/analysis parameters.csv','w') as test_outputfile:\n",
    "    writer = csv.writer(test_outputfile, delimiter = ',')\n",
    "    writer.writerows([['raw data parameters']])\n",
    "    writer.writerows([['num of plates:', , , '%i' % PLATES]])\n",
    "    writer.writerows([['num of rows:', , , '%i' % PLATE_ROWS]])\n",
    "    writer.writerows([['num of cols:', , , '%i' % PLATE_COLS]])\n",
    "    writer.writerows([['num of replicates:', , , '%i' % NUM_REPL]])\n",
    "    writer.writerows([['num of control and exp groups:', , , '%i' % NUM_GROUPS]])\n",
    "    writer.writerows([['loc of first row:', , , '%i' % (FIRSTROW + 1)]])\n",
    "    writer.writerows([['loc of last row:', , , '%i' % (LASTROW)]])\n",
    "    writer.writerows([['loc of first col:', , , '%i' % (FIRSTCOL + 1)]])\n",
    "    writer.writerows([['loc of last col:', , , '%i' % (LASTCOL)]])\n",
    "\n",
    "    writer.writerows([['analysis parameters']])\n",
    "    writer.writerows([['num of k nearest neighbors:', , , '%i' % K_NEAREST_NEIGHBORS]])\n",
    "    writer.writerows([['num of polish iterations:', , , '%i' % NUM_POLISH_ITER]])\n",
    "    writer.writerows([['median abs dev scaling is:', , , '%i' % MAD_SCALING_CONST]])\n",
    "    writer.writerows([['median abs dev threshold during hit selection:', , , '%i' % THRESHOLD]])\n",
    "    writer.writerows([['significance level during hit selection:', , , '%i' % SIGMA]])\n",
    "    writer.writerows([['false discovery rate:', , , '%i' % FDR]])\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
